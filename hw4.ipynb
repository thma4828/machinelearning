{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "535f16b66d12363ace62e46632df2fd7",
     "grade": false,
     "grade_id": "cell-527cc2929f6c6660",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### HW 4: Naive Bayes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2870fc92e6d85aa81b586a60ec54b2d",
     "grade": false,
     "grade_id": "cell-ff2d107bf94e97a9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "\n",
    "# Part 1 - Theory / Written Problems\n",
    "\n",
    "\n",
    "### [16 points] Problem 1 - Naive Bayes for Tennis Prediction \n",
    "***\n",
    "\n",
    "Suppose you are trying to learn a person's decision whether to play tennis or not on a given day using features corresponding to precipitation forecast, temperature, humidity, and wind. You're given the following training data: \n",
    "\n",
    "$$\n",
    "\\begin{array}{c|c|c|c|c}\n",
    "\\textbf{Forecast} & \\textbf{Temp} & \\textbf{Humidity} & \\textbf{Wind} & \\textbf{PlayTennis} \\\\\n",
    "\\hline \n",
    "\\textrm{sunny} & \\textrm{hot} & \\textrm{high} & \\textrm{weak} & \\textbf{No} \\\\ \n",
    "\\textrm{sunny} & \\textrm{hot} & \\textrm{high} & \\textrm{strong} & \\textbf{No} \\\\ \n",
    "\\textrm{overcast} & \\textrm{hot} & \\textrm{high} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{mild} & \\textrm{high} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{cool} & \\textrm{normal} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{cool} & \\textrm{normal} & \\textrm{strong} & \\textbf{No} \\\\ \n",
    "\\textrm{overcast} & \\textrm{cool} & \\textrm{normal} & \\textrm{strong} & \\textbf{Yes} \\\\ \n",
    "\\textrm{sunny} & \\textrm{mild} & \\textrm{high} & \\textrm{weak} & \\textbf{No} \\\\ \n",
    "\\textrm{sunny} & \\textrm{cool} & \\textrm{normal} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{mild} & \\textrm{normal} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{sunny} & \\textrm{mild} & \\textrm{normal} & \\textrm{strong} & \\textbf{Yes} \\\\ \n",
    "\\textrm{overcast} & \\textrm{mild} & \\textrm{high} & \\textrm{strong} & \\textbf{Yes} \\\\ \n",
    "\\textrm{overcast} & \\textrm{hot} & \\textrm{normal} & \\textrm{weak} & \\textbf{Yes} \\\\ \n",
    "\\textrm{rainy} & \\textrm{mild} & \\textrm{high} & \\textrm{strong} & \\textbf{No} \\\\ \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "**Part A (2pt)**: Estimate the **unconditional** probabilities $p(\\textrm{PlayTennis=Yes})$ and $p(\\textrm{PlayTennis=No})$ from the training data. These are also called the \"prior\" probabilities. **Note in all the problems that follow, unless directed otherwise, you must show/describe how you arrived at your answer, even if the method is simple. Simply writing  the numeric answer is not enough.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6992779ca5b743987a6bd7324c0ef479",
     "grade": true,
     "grade_id": "cell-e1f2324117ff87fd",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "$p(\\textrm{PlayTennis=Yes}) = \\frac{9}{14}$ and $p(\\textrm{PlayTennis=No}) = \\frac{5}{14}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9f61741ce49ab061d0755f6857c2fe76",
     "grade": false,
     "grade_id": "cell-a2c17a6972949b0f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part B (6 pt)**: \n",
    "1. For each feature, estimate the class-conditional probabilities $p(\\textrm{feature value} \\mid \\textrm{PlayTennis} = \"Yes\" )$ and $p(\\textrm{feature value} \\mid \\textrm{PlayTennis} = \"No\" )$. Show your work.\n",
    "\n",
    "2. For a given probability mass function for a discrete random variable $X$, the sum of the probabilities over the range of values the variable can take must add up to 1. Is the following equation true? Why or why not?\n",
    "\n",
    "$$  p(\\textrm{Forecast=Sunny} | \\textrm{Humidity = high} ) +  p(\\textrm{Forecast=Overcast} | \\textrm{Humidity = low} ) +  p(\\textrm{Forecast=rainy} | \\textrm{Humidity = normal} ) = 1 $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c02c0014bb4493b3d07db85039468be",
     "grade": true,
     "grade_id": "cell-2584fb81679aaf31",
     "locked": false,
     "points": 6,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "2. NO, this is not the case,the probability would be one if we summed up \n",
    "$p(Forecast=Sunny | Humidity=high) + p(Forecast=Overcast | Humidity = high) + p(Forecast=rainy | Humidity = high)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8e0315f64323611f7e176c184de3a3ef",
     "grade": false,
     "grade_id": "cell-dc5403343700c01f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part C (4 pt)**:\n",
    "    \n",
    "1. Consider the Naive Bayes model where the $x_i$ features are discrete random variables (also known as Multinomial Naive Bayes): \n",
    "\n",
    "$$ p(y \\mid x_1, ..., x_n) \\approx p(y) \\Pi_i^n p(x_i | y) $$\n",
    "\n",
    "In this case, our prediction is made by:\n",
    "\n",
    "$$ \\hat{y}(x) = \\mathop{\\mathrm{argmax}}_y p(y) \\Pi_i^n p(x_i \\mid y) $$\n",
    "\n",
    "What will happen if we receive a query point $x$ such that the $i$-th feature $x_i$ contains a value not in the original dataset. Suppose this value is $v$. What is $p(x_i=v \\mid y)$? Such a case may occur if, for example, our features are words in a sentance. Then a rare word may appear that was not in the original training dataset. Assume you trained your model only on words in the training data using exact counts of features to estimate the requisuite probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7eb90b9c15f02f23d066d3317ec07802",
     "grade": true,
     "grade_id": "cell-1c0e5ee75b537eef",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "Then we must estimate the label based off of the posterior! \n",
    "\n",
    "$p(z | x_{i} ; \\theta) = \\frac{p(x_{i} | z ; \\theta)p(z ; \\theta)}{\\Sigma_{z^{'} = 1}^{k} p(x_{i} | z^{'}); \\theta)p(z^{'} ; \\theta) }$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part D (4 pts)**: \n",
    "One way to solve the problem you discovered in (1) is to artificially inflate the count of all possible words by some small number. So in an NLP dataset, all possible words would have at least some minimum count when calculating the above model. We can also introduce an `<unk>` word with a minimum count that is assigned to any words that we didn't account for at train-time. Inflating the counts results in so-called **Laplace Smoothing**:\n",
    "\n",
    "$$p(x_i =j| y_i=y) \\approx \\frac{N_{yij}+ \\alpha }{N_y + \\alpha D} $$\n",
    "\n",
    "where $N_{y}$ is the total count of all samples with label $y$, $N_{yij}$ is the total count of all samples such that $x_i=j$ and $y_i=y$, and $D$ is the number of features. $\\alpha$ is an adjustable parameter and can be thought of as a an additional factor added to each feature count, although it need not be an integer.\n",
    "\n",
    "Assume a Naive Bayes model with Laplace Smoothing such that $\\alpha=2$. What would your Naive Bayes model predict for the following two weather conditions? Show all work.  \n",
    "- **Forecast**=overcast, **Temp**=cool, **Humidity**=high, **Wind**=weak  \n",
    "- **Forecast**=sunny, **Temp**=hot, **Humidity**=normal, **Wind**=strong  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32bf31ed81bed731f087d4980b964d89",
     "grade": true,
     "grade_id": "cell-762f9fd576f071b9",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "244fe72bfacc1d7a4612322e9a2f28fa",
     "grade": false,
     "grade_id": "cell-298b5a356f5fd701",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### PART 2 [15 points] Implementing Naive Bayes for Text Classification \n",
    "***\n",
    "\n",
    "In this problem you'll implement a general Discrete Naive Bayes class for text classification. Your tasks will be to implement `train`, `predict_log_score`, and `predict` routines to learn the Naive Bayes model parameters from a collection on text and make predictions on unseen validation data. \n",
    "\n",
    "The skeleton for the `TextNB` class is below. \n",
    "\n",
    "**Important Note**: In Problem 3 we'll be using the `TextNB` class to make predictions about Twitter data.  Since real-world text data typically has a large number of features, you'll want to make your implementation reasonably efficient so that your experiments in Part 3 don't take forever. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ast\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b918713fe93e088de73222c7b5c0f342",
     "grade": false,
     "grade_id": "cell-81e29b259b0173d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class TextNB:\n",
    "    def __init__(self, text_train, y_train, alpha=1.0):\n",
    "        \"\"\"\n",
    "        :param text_train: a list or ndarray of text strings to use as training data \n",
    "        :param y_train: an ndarray of true labels associated with the text data \n",
    "        :param alpha: the Laplace smoothing parameter \n",
    "        \"\"\"\n",
    "        \n",
    "        # store training data \n",
    "        self.text_train = text_train \n",
    "        self.y_train = y_train \n",
    "        \n",
    "        # store smoothing parameter\n",
    "        self.alpha = alpha \n",
    "        \n",
    "        # get number of classes \n",
    "        self.num_classes = len(np.unique(y_train))\n",
    "        \n",
    "        # initialize vocab to feature map \n",
    "        self.vocab = dict() \n",
    "        \n",
    "        # initialize class counts \n",
    "        self.class_counts = np.zeros(self.num_classes, dtype=int)\n",
    "        \n",
    "        # initialize feature counts (Note, will need to update this with the correct\n",
    "        # number of columns during the training process)\n",
    "        self.feature_counts = np.zeros((self.num_classes, 0), dtype=int)\n",
    "        \n",
    "        # Any extra fields you want go here\n",
    "        # YOUR CODE HERE\n",
    "        self.word_counter = {}\n",
    "        \n",
    "        self.V = CountVectorizer()\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Learn the vocabularly, class_counts, and feature counts from the training data \n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        for word in self.text_train:\n",
    "            if word in self.word_counter:\n",
    "                self.word_counter[word] += 1\n",
    "            else:\n",
    "                self.word_counter[word] = 1\n",
    "        \n",
    "        X = self.V.fit(self.text_train)\n",
    "        for i, row in enumerate(X):\n",
    "            self.vocab[self.text_train[i]] = row\n",
    "            \n",
    "                    \n",
    "                    \n",
    "    def predict_log_score(self, text_str):\n",
    "        \"\"\"\n",
    "        Get the log-probability score for each class\n",
    "        for a query string\n",
    "        \n",
    "        :param text_str: a single string of text to compute the log_score for \n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    \n",
    "    def predict(self, text_list):\n",
    "        \"\"\"\n",
    "        Predict the class of each example in text_list  \n",
    "        \n",
    "        :param text_list: a list or ndarray of text strings to make predictions on \n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d262235a4f20151388dba619ac247a24",
     "grade": false,
     "grade_id": "cell-00ea243e102e6fa2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Not that in all of the following you may NOT use scikit-learn NaiveBayes classes.**\n",
    "\n",
    "**Part A**: Complete the `train` function in the `TextNB` class to prepare to make Naive Bayes predictions using Laplace smoothing.  In this routine you will need to populate the following data structures: \n",
    "\n",
    "`self.vocab`: A Python dictionary that maps distinct terms found in the training set to unique indices in $\\{0, 1, \\ldots, |V|-1\\}$.  This will allow us to quickly look up frequency counts for an encountered term in a Numpy array. Note that while the data is fairly clean (We've removed punctuation and made all characters lowercase) you should take care that you're not accidentally counting whitespace in the vocabulary. \n",
    "\n",
    "`self.class_counts`: A 1D Numpy array of length `self.num_classes` which counts the number of documents in the training set that belong to each class. \n",
    "\n",
    "`self.feature_counts`: A 2D Numpy array of dimensions `self.num_classes` $\\times$ $|V|$. The $(c,k)$-entry in this array should be the number of times that term $k$ appears in documents belonging to class $c$. Note that we're using the Bag-of-Words approach here, so if a term appears multiple times in a single document, each instance of that term should be counted.  \n",
    "\n",
    "When you think you're done, execute the following unit test. 2 are available for you, one is hidden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51af6e50bbd2f6d58dcbcc6227117e62",
     "grade": true,
     "grade_id": "cell-e925aff87ac8cf0b",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def testVocab(): \n",
    "        \"\"\"\n",
    "        test vocabulary \n",
    "        \"\"\"\n",
    "        text_train = np.array([\"work buy  money\", \"prince opportunity viagra\", \"fly   buy prince\", \"money buy fly\", \"fly home prince\"])\n",
    "        y_train = np.array([0, 1, 1, 1, 0], dtype=int)\n",
    "        nb = TextNB(text_train, y_train)\n",
    "        nb.train()\n",
    "        vocab_set = set([\"work\", \"buy\", \"money\", \"prince\", \"opportunity\", \"viagra\", \"fly\", \"home\"])\n",
    "        for w in vocab_set:\n",
    "            assert(w in nb.vocab)\n",
    "            assert(nb.vocab[w] in set(list(range(8))))\n",
    "            \n",
    "def testClassCounts(): \n",
    "        \"\"\"\n",
    "        test counts of documents in each class \n",
    "        \"\"\"\n",
    "        text_train = np.array([\"work buy  money\", \"prince opportunity viagra\", \"fly   buy prince\", \"money buy fly\", \"fly home prince\"])\n",
    "        y_train = np.array([0, 1, 1, 1, 0], dtype=int)\n",
    "        nb = TextNB(text_train, y_train)\n",
    "        nb.train()\n",
    "        vocab_set = set([\"work\", \"buy\", \"money\", \"prince\", \"opportunity\", \"viagra\", \"fly\", \"home\"])\n",
    "        assert(nb.class_counts[0] == 2)\n",
    "        assert(nb.class_counts[1] == 3)\n",
    "\n",
    "testVocab()\n",
    "testClassCounts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c38783a5391c18d6d842fd0a042ddb6f",
     "grade": true,
     "grade_id": "cell-acbeb3b7ea9f7c20",
     "locked": true,
     "points": 2,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT DELETE THIS CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "70100d4460002dc59db46c9581a58438",
     "grade": false,
     "grade_id": "cell-a70e28244937c0e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part B**: Complete the `predict_log_score` function in the `TextNB` class to take in a single string of text and compute the associated log-score for each class. For now, you should use Laplace smoothing with $\\alpha=1$ for both the class-priors and the class-conditional probabilities.  In **Problem 3** we'll experiment with different variants of Laplace smoothing, so if you like you can read ahead now and implement the general version of Laplace smoothing from the beginning.  \n",
    "\n",
    "**Note**: For simplicity and testing purposes, do not implement an `UNK` feature.  Instead, if you encounter a term not in the vocabulary you can safely ignore it. \n",
    "\n",
    "When you think your `predict_log_score` function is working well, execute the following unit test. Some unit tests are hidden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fb711c11d8c77ff238a75c4a1284150b",
     "grade": true,
     "grade_id": "cell-58f78a77269133ab",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def testPrediction(): \n",
    "    \"\"\"\n",
    "    test vocabulary \n",
    "    \"\"\"\n",
    "    text_train = np.array([\"work buy  money\", \"prince opportunity viagra\", \"fly   buy prince\", \"money buy fly\", \"fly home prince\"])\n",
    "    y_train = np.array([0, 1, 1, 1, 0], dtype=int)\n",
    "    nb = TextNB(text_train, y_train, alpha=1.0)\n",
    "    nb.train()\n",
    "    test1 = \"money money money\"\n",
    "    test2 = \"money fly buy\"\n",
    "    logits1 = nb.predict_log_score(test1)\n",
    "    logits2 = nb.predict_log_score(test2)\n",
    "    print(logits1, logits2)\n",
    "    diff = np.abs(logits1-np.array([-6.685, -6.9798]))\n",
    "    assert(diff.sum()<1e-4)\n",
    "    diff = np.abs(logits2-np.array([-6.685, -6.1689]))\n",
    "    assert(diff.sum()<1e-4)\n",
    "    \n",
    "testPrediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0d68869e8a37c042445265d45be8ce12",
     "grade": false,
     "grade_id": "cell-e32a645bef8f4e0c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part C**: Finally, implement the `predict` method to take in a list or ndarray of text data, call `predict_log_score`, and return a vector of predicted labels. \n",
    "\n",
    "When you think you're done, execute the following unit test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eda99cc7fdb8db00f5542a5744261b1c",
     "grade": true,
     "grade_id": "cell-436849047d2409a9",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def testPredict(): \n",
    "    text_train = np.array([\"work buy  money\", \"prince opportunity viagra\", \"fly   buy prince\", \"money buy fly\", \"fly home prince\"])\n",
    "    y_train = np.array([0, 1, 1, 1, 0], dtype=int)\n",
    "    nb = TextNB(text_train, y_train)\n",
    "    nb.train() \n",
    "    yhat = nb.predict(text_train)\n",
    "    assert(len(yhat)== 5)\n",
    "    assert(all(yh==yi for yh, yi in zip(yhat, y_train)))\n",
    "        \n",
    "testPredict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cf01357f9b33cc67a2269e559026fdcd",
     "grade": false,
     "grade_id": "cell-cb39da44cc9b3251",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### [12 points] Problem 3: Predicting the Sentiment of Tweets sent from Passengers to Airlines \n",
    "***\n",
    "\n",
    "In this problem you'll use the `TextNB` class you wrote in **Problem 2** to make predictions about the sentiment of tweets sent by passengers to airlines.  Execute the following cell to load the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(path='airline_tweets.pklz'):\n",
    "    f = open('airline_tweets.pklz','rb')\n",
    "    text_train, y_train, text_valid, y_valid, text_all, y_all = np.load(f, allow_pickle=True)\n",
    "    f.close()\n",
    "    return {\n",
    "        'train':{\n",
    "            'tweets': text_train,\n",
    "            'labels': y_train\n",
    "        },\n",
    "        'valid': {\n",
    "            'tweets': text_valid,\n",
    "            'labels': y_valid\n",
    "        },\n",
    "        'all': {\n",
    "            'tweets': text_all,\n",
    "            'labels': y_all\n",
    "        }\n",
    "    }\n",
    "\n",
    "# This should be the only global variable you use in your cells.\n",
    "# Treat it as immutable. Do not access it directly in any function, it only gets\n",
    "# passed in.\n",
    "global_dataset = get_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3a9aeb8a7719d4f6c3dcf164b097ef15",
     "grade": false,
     "grade_id": "cell-53c119a1daedc563",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part A (4Pts)**: Explore the data and fill in the below function toanswer the following questions: \n",
    "\n",
    "- How many total examples are there in the training set?\n",
    "- How many total examples are there in the validation set?\n",
    "- Which binary label ($\\{0,1\\}$) corresponds to tweets with positive sentiment?\n",
    "- What percentage of tweets in the training set have true positive sentiment? \n",
    "- What percentage of tweets in the validation set have true positive sentiment? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell is for scratch space to explore the data before filling in the below function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e3e476489db60db516ed3b70017a2be",
     "grade": false,
     "grade_id": "cell-1a9a6c6a18b5e01b",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def get_dataset_info(dataset):\n",
    "    '''\n",
    "    dataset - the loaded dataset\n",
    "    \n",
    "    returns:\n",
    "        np.array of size 5 with the elements from above (in order)\n",
    "    '''\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "762cd3e4334e4e9f87ce5e74a96f5389",
     "grade": true,
     "grade_id": "cell-44d6f1137fdafd2f",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT USE OR DELETE THIS CELL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f12b5f0b5ebe80820a08bfc96f1ee17",
     "grade": false,
     "grade_id": "cell-b9ff55fe37bb90ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part B (4 Pts)**: Use your `TextNB` class to learn a Naive Bayes classifier for the airline Twitter data.  What accuracy do you achieve on the training set and what accuracy, precision, and recall do you achieve on the validation set? \n",
    "\n",
    "You may use `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7ed382124d0b35510aa0d0c5675315cf",
     "grade": false,
     "grade_id": "cell-f5c5782edb8ff5d8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def build_and_evaluate(dataset, alpha=1.0):\n",
    "    '''\n",
    "    args - \n",
    "        dataset: the dataset object\n",
    "        alpha: the alpha value (for Laplace smoothing) given to TextNB\n",
    "    \n",
    "    Returns an np.array of 6 numbers: \n",
    "        train set accuracy,\n",
    "        train set precision, \n",
    "        train set recall, \n",
    "        valid set accuracy, \n",
    "        valid set precision,\n",
    "        valid set recall\n",
    "        \n",
    "    You should use your TextNB class\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0273605aedb3532c8cc184ebf20b3f5",
     "grade": true,
     "grade_id": "cell-bb7c76ec22048a6f",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT USE OR DELETE THIS CELL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e59018303579cfea8501262840ea7f2",
     "grade": false,
     "grade_id": "cell-1593aa1b6d1c82b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "**Part D**: Write a procedure to perform $K$-Folds cross-validation on the entire data set (`train_all` and `y_all`) to estimate the accuracy of your NB classifier for various values of $\\alpha$ and make a plot showing your results.  \n",
    "\n",
    "To do the partitioning into folds we recommend leveraging sklearn's [StratifiedKFold](https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation-iterators-with-stratification-based-on-class-labels) routine.  The documentation demonstrates how it can be used.  \n",
    "\n",
    "For your plot, use at least $K=5$ folds and at least $5$ different values of $\\alpha$ between $0.1$ and $1.5$ (or higher).  Which value of $\\alpha$ seems to perform the best? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1f4f6ce8423693b3f5cd2a0c4d374ab7",
     "grade": true,
     "grade_id": "cell-7ef6a941fdcb5f81",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def kfold_validation(dataset, alpha_vals=np.linspace(0.1, 1.5, 5)):\n",
    "    '''\n",
    "    Use your build_and_evaluate function to perform kfold_validation\n",
    "    to select the alpha parameter.\n",
    "    \n",
    "    You should return a A x 6 array, 1 row for each alpha value,\n",
    "    and 1 column for each metric returned from build_and_evaluate\n",
    "    '''\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "706f4b4658f0d29990a475230357fc48",
     "grade": true,
     "grade_id": "cell-1bbf1c7c45a697ea",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3d656679c97837202a4f6ae734e7dc6",
     "grade": true,
     "grade_id": "cell-69661fdbdfd6efcb",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Do not modify this cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
