{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dc67d7950defb3021126dca654cfdf7a",
     "grade": false,
     "grade_id": "cell-95f3e215249e474e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# HW 6 - Neural Networks\n",
    "\n",
    "In this homework, you will build and implement a deep neural network, and then study the effects of architecture and initialization on training convergence and performance.\n",
    "\n",
    "The neural networks we will study in this homework are purely **feed-forward, densely connected** neural networks, also called multi-layer perceptrons (MLP). In other words, this homework does not cover convolution or recurrence, which are common in many models. Nevertheless, Google reports the majority of its neural network workloads in production are MLPs, so they are highly relevant. \n",
    "\n",
    "You will perform the following fundamentals:\n",
    "1. Implement a feed-forward neural network\n",
    "2. Implement the back-propogation algorithm \n",
    "3. Implement the softmax-cross-entropy-loss combination, a very common loss function for classification.\n",
    "\n",
    "You will perform experiments to understand the following critical aspects of neural network training: convergence, Initialization, and complexity of representation.\n",
    "\n",
    "You will need to do the problems **in order**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "785fb294674fabafaadca68ae1da0b7c",
     "grade": false,
     "grade_id": "cell-638d3ae50f41b79c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Written Problems\n",
    "\n",
    "\n",
    "### Notation used Throughout the Notebook\n",
    "\n",
    "$X = \\mathbb{R}^d$ is the space in which your **training examples** live.\n",
    "\n",
    "We will let $x_i$ denote a single training example.\n",
    "\n",
    "$Y=\\{0,1,...,K-1\\}$ is the space in which your **training example labels** live. There are $K$ classes.\n",
    "\n",
    "The training data is a set $T$ which consists of $N$ examples:\n",
    "$$ T = \\{ (x_0, y_0), ..., (x_N, y_N) \\quad | \\quad x_i \\in X, y_i \\in Y \\} $$ \n",
    "\n",
    "$F: X \\rightarrow \\mathbb{R}^K $ is the function which describes your neural network. The output of the network is $K$ dimensional because it outputs a unit-less score called a **logit** for each class.\n",
    "\n",
    "The neural network consists of a set of layers. A layer accepts the output, a vector, from the previous layer, performs a matrix multiply using its **weight matrix**, adds its **bias vector**, applies the ReLU non-linearity pointwise to the result, and outputs that result to the next layer.  \n",
    "\n",
    "**The exception to the above is the last layer of the neural network, which outputs the logits. \n",
    "It does not have an activation applied**.\n",
    "\n",
    "The layers of the neural network have indices from $0$ (the layer which accepts in the input data $x_i$) to $L-1$.\n",
    "$L$ is the number of layers.\n",
    "\n",
    "$H^l$ is the size of the **output** of the $l$-th layer. $H^{-1}$ is the size of the input, $d$. \n",
    "\n",
    "$W^l \\in \\mathbb{R}^{H^{l-1} \\times H^{l}}$ is the weight matrix for layer $l$. \n",
    "\n",
    "$b^l \\in \\mathbb{R}^{H^l}$ is the bias vector for layer $l$. \n",
    "\n",
    "$(\\cdot)^+$ is the ReLU function, which can accept a matrix or vector of any size, and is applied element-wise on the matrix. So if $x^T = [1,-1,2,-4]$, then $(x)^+ = [1,0,2,0]$.\n",
    "\n",
    "$F^{l}(x_i)$ is the **output post-activation (after ReLU applied)** of the $l$-th layer. \n",
    "\n",
    "$f^{l}(x_i)$ is the **output pre-activation (before ReLU applied)** of the $l$-th layer. \n",
    "\n",
    "\n",
    "$L: \\mathbb{R}^K \\times Y \\rightarrow \\mathbb{R}^+$ is the loss function which takes in **logit scores** and **labels** and outputs a non-negative number, the **loss**. We describe the loss function below.\n",
    "\n",
    "In any formula below, $\\exp(a)$ means $e^{a}$.\n",
    "\n",
    "Matrices and vectors have components with indices that start at 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c947aa1ddf288da811241a46ba6daec",
     "grade": false,
     "grade_id": "cell-638d3ae50f4df",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 1 - Simple Forward Propogation (3 points):\n",
    "\n",
    "Using NumPy is fine in this problem, but don't include the NumPy code, and you must type your answers in Latex.\n",
    "\n",
    "Suppose the neural network has $2$ layers, not including the loss function. Let $H^0 = 2$, $H^1 = 3$. You are given the following:\n",
    "\n",
    "$\n",
    "W^{0} = \n",
    "\\begin{bmatrix}\n",
    "1.62 & -0.61 \\\\\n",
    "-0.53 & -1.07\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "$\n",
    "W^{1} = \n",
    "\\begin{bmatrix}\n",
    "0.87 & -2.3 \\\\\n",
    "1.74 & -0.76 \\\\\n",
    "0.32 & -0.25\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "$ b^0 = \\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix} $\n",
    "\n",
    "$ b^1 = \\begin{bmatrix} -0.1 \\\\ 0.1 \\\\ 0.2 \\end{bmatrix} $\n",
    "\n",
    "$ x_i = \\begin{bmatrix} 0.5 \\\\ 0.1 \\end{bmatrix} $\n",
    "\n",
    "\n",
    "1. Write the equations (without numbers, only symbols, for $F^{0}(x_i)$ and $F^{1}(x_i)$. You can use the term \"$F^{0}(x_i)$\" in the second. \n",
    "\n",
    "2. Calculate the numerical values for $f^{0}, F^{0}$ and $f^{1}$. and  $F^{1}$. As specified in the notation above, the last layer $F^{1}$ does not receive an activation, thus $f^{1}=F^{1}$.\n",
    "\n",
    "3. What is the value of $K$ for this network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "10191ac690a2abf3bbc36bd2eb09a821",
     "grade": true,
     "grade_id": "cell-d92a29521c3e4610",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": false
   },
   "source": [
    "1. \n",
    "    - $F^{0}(x_i)$ = $(W^{0}\\space \\dot \\space x_{i}$ $+$ $b^{0})^{+}$\n",
    "    - $F^{1}(x_i)$ = $W^{1}\\space \\dot \\space (F^{0}(x_i))$ $+$ $b^{1}$\n",
    "    \n",
    "2.  \n",
    "    - $f^{0}$ $=$ $[1.749, \\space  0.628]^T$\n",
    "    - $F^{0}$ $=$ $[1.749, \\space  0.628]^T$\n",
    "    - $f^{1}$ $=$ $[-0.02277, \\space  2.66598, \\space 0.60268]^T$\n",
    "    - $F^{1}$ $=$ $[-0.02277, \\space  2.66598, \\space 0.60268]^T$\n",
    "3. \n",
    "    -$K$ $=$ $3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "023c94a58a0d4c4b0eb0b12550c75383",
     "grade": true,
     "grade_id": "cell-0ff951930ca4e690",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.87 -2.3 ]\n",
      " [ 1.74 -0.76]\n",
      " [ 0.32 -0.25]]\n",
      "[-0.02277  2.66598  0.60268]\n"
     ]
    }
   ],
   "source": [
    "# Do not delete this cell.\n",
    "# YOUR CODE HERE\n",
    "import numpy as np\n",
    "\n",
    "w_0 = np.array([[1.62, -0.61],[-0.53, -1.07]])\n",
    "xi = np.array([0.5, 0.1])\n",
    "b0 = np.array([1, 1])\n",
    "f0 = np.dot(w_0, xi) + b0\n",
    "\n",
    "w_1 = np.array([[0.87, -2.3],[1.74, -0.76],[0.32, -0.25]])\n",
    "print(w_1)\n",
    "b1 = np.array([-0.1, 0.1, 0.2])\n",
    "\n",
    "f1 = np.dot(w_1, f0) + b1\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eaa4fef08a548eb1530545d556a5413d",
     "grade": false,
     "grade_id": "cell-ad6868974200d076",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 2 - Softmax (2 points)\n",
    "\n",
    "Using NumPy is fine in this problem, but don't include the NumPy code, and you must type your answers in Latex.\n",
    "\n",
    "Our network above outputs a set of $k$  scores called $logits$. We want to reward the network when it gives a high score for the correct class and penalize it for giving high scores for the wrong class. We do this by first applying the **softmax** function to the logits to turn them into probabilities, then calculating the **cross-entropy** between the softmax output and the **label distribution**.\n",
    "\n",
    "The **softmax** function $S: \\mathbb{R}^K \\rightarrow \\mathbb{R}^K$ takes our **logits** and turns them into a vector of **probabilities** that sum to 1. It is a generalization of the sigmoid function we have already seen in logistic regression.\n",
    "\n",
    "Softmax is best explained with an example. Let $v = \\begin{pmatrix} v_1 \\\\ v_2 \\\\ v_3 \\end{pmatrix} $ be the logits vector for $K=3$. Then\n",
    "$S(v)$ is:\n",
    "\n",
    "$$ S(v) = \\frac{1}{C} \\begin{pmatrix} \\exp(v_1) \\\\ \\exp(v_2) \\\\ \\exp(v_3) \\end{pmatrix} $$\n",
    "\n",
    "Where $C = \\sum_{j=1}^K \\exp(v_j)$.\n",
    "\n",
    "1. Calculate $S(F_1(x_i))$ from the output of $F_1(x_i)$ you got in problem 1. Remember the last layer of $F$ does not get an activation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d7da52a0e49ef038235bef5d54c7d22f",
     "grade": true,
     "grade_id": "cell-639eaa1258ea5e47",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": false
   },
   "source": [
    "$S(F_1(x_i))$ $=$ $[0.31187539, \\space 0.36469044, \\space 0.32343417]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bf51953faa26528545b095737e7ae4f9",
     "grade": true,
     "grade_id": "cell-4e38219aff3a4736",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "import math\n",
    "F1 = f1\n",
    "def softmax(v):\n",
    "    return np.exp(v) / sum([math.exp(vi) for vi in v])\n",
    "\n",
    "sv = softmax(F1)\n",
    "np.sum(sv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47aed1d1b98b93376a5fa2a9208748cc",
     "grade": false,
     "grade_id": "cell-640c69b096e6de96",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 3 - Cross Entropy Loss (4 Points)\n",
    "\n",
    "Now that we converted the logits to probabilities, we need to formualte a loss with respect to the label. Remember, for a datapoint $(x_i, y_i)$, the label $y_i$ takes on a value from $\\{0, 1, ..., K-1\\}$. \n",
    "\n",
    "Let the **one-hot** vector representation of $y_i$ be given by $h(y_i)$. The one-hot representation has zeros everywhere except for the index indicated by $y_i$. So if $K=3$ and $y_i=1$, then \n",
    "\n",
    "$h(y_i) = [0, 1, 0]$\n",
    "\n",
    "Note, for any $y_i$, the vector $h(y_i)$ can be though of as the emprical probability over the labels.\n",
    "\n",
    "The cross-entropy loss function $L: \\mathbb{R}^K \\times Y \\rightarrow \\mathbb{R}^+$ is given by:\n",
    "\n",
    "$$ L(F(x_i), y_i) = - \\sum_{j=1}^{K} h(y_i)_j \\log S(F(x_i))_j $$\n",
    "\n",
    "Where $h(x_i)_j$ is the $j$-th component of $h(x_i)$ and likewise for $S(F(x_i))$. \n",
    "\n",
    "1. Simplify the equation for $L$ above given what we know about $h$. \n",
    "\n",
    "2. Assuming $y_i = 2$ for our example so far, calculate the Cross-Entropy loss from the above Softmax probabilities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1509a7facd32b309f9c4516dd1892a7",
     "grade": true,
     "grade_id": "cell-0b4e7dca424e1bae",
     "locked": false,
     "points": 4,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": false
   },
   "source": [
    "1. $L(F(x_i),\\space  y_i)$ $=$ $-\\log{S(F(x_i))}_{y_i}$ \n",
    "2. $L(F(x_i), \\space 2)$ $=$ $1.1287596901651669$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "855a3975de893f712474d5cd68ec868e",
     "grade": true,
     "grade_id": "cell-0232481dd5096153",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.241446108913464"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "yi = 2\n",
    "-1 * np.log(sv[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9a544432cc5a8c12a0d79d841ba8a3dc",
     "grade": false,
     "grade_id": "cell-958d924deefe01c6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Problem 4 - Backpropogation (7 points, Extra Credit)\n",
    "\n",
    "In the following, we our going to assume that we are using SGD with a batch size of $B$. Let $L_i$ be our loss function with respect to a single example $(x_i, y_i)$. Thus, from Problem 3, we have:\n",
    "\n",
    "$$ L_i = - \\sum_{j=1}^{K} h(y_i)_j \\log S(F(x_i))_j $$ \n",
    "\n",
    "Our full loss function for our batch is:\n",
    "\n",
    "$$ L = \\frac{1}{B} \\sum_{i=1}^{B} L_i $$\n",
    "\n",
    "$$ L = -\\frac{1}{B} \\sum_{i=1}^{B} \\sum_{j=1}^{K} h(y_i)_j \\log S(F(x_i))_j $$\n",
    "\n",
    "\n",
    "We will now calculate a series of derivatives in the order they are calculated in backpropogation. You need to know these so you can ensure your coding work is correct.\n",
    "\n",
    "**In all of the following, we only ask you to calculate the derivative with respect to one variable (i.e. Calc 1 derivatives).**\n",
    "\n",
    "Using our running example so far, find each of the following. Calculate the **symbolic/analytic** answer (without numbers plugged in), then calculate the **numerical** answer. Show your work.\n",
    "\n",
    "1. Calculate the derivative of $L$ with respect to $F^1(x_i)_1$\n",
    "\n",
    "2. Calculate the derivative of $F^1(x_i)_1$ with respect to $f^1(x_i)_1$\n",
    "\n",
    "3. Calculate the derivative of $f^1(x_i)_1$ with respect to $W^{1}_{(1,2)}$ \n",
    "\n",
    "4. Calculate the derivative of $f^1(x_i)_1$ with respect to $b^{1}_{1}$ \n",
    "\n",
    "5. Calculate the derivative of $f^1(x_i)_1$ with respect to $F^{0}_{2}$ \n",
    "\n",
    "6. Calculate the derivative of $f^0(x_i)_2$ with respect to $W^{0}_{(2,1)}$. \n",
    "\n",
    "7. Calculate the derivative of $f^0(x_i)_2$ with respect to $b^{0}_{2}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f4018f1d063e8b9a41c228ecc611e4ee",
     "grade": true,
     "grade_id": "cell-c992e395860280c9",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": false
   },
   "source": [
    "1. \n",
    "    - for $(x_i, y_i)$ $L_i$ $=$ $-\\log{S(F^1(x_i))}_{y_i}$ \n",
    "    - and we know that for $B$ examples $(x_i, y_i)$\n",
    "        - $L$ $=$ $\\Sigma_{i=1}^B$ $-\\log{S(F^1(x_i))}_{y_i}$ \n",
    "    - therefore the derivative of $L$ with respect to $F^1(x_i)_1$ is:\n",
    "        - $-\\Sigma_{i=1}^B$ $\\frac{1}{S(F^1(x_i)_1}\\space \\dot \\space \\frac{dS}{dF^1{x_i}_1}$\n",
    "2. The derivative of $F^1(x_i)_1$ with respect to $f^1(x_i)_1$ = 1\n",
    "\n",
    "3. the derivative of $f^1(x_i)_1$ with respect to $W^{1}_{(1,2)}$ = $0$\n",
    "\n",
    "4.  the derivative of $f^1(x_i)_1$ with respect to $b^{1}_{1}$  = $1$\n",
    "\n",
    "5. the derivative of $f^1(x_i)_1$ with respect to $F^{0}_{2}$  = $[0, 1]^T$\n",
    "\n",
    "6. zero\n",
    "\n",
    "7. one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "509289444eaacd34329914a9eade0358",
     "grade": false,
     "grade_id": "cell-e03f3a1527a446fc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 2: Implementation of Neural Network with Backpropogation (10 points)\n",
    "\n",
    "In this problem, we will use the following code to build our neural network and conduct training. We do this in the style of PyTorch reverse-mode auto differentiation, exactly as discussed in class. For each function, your task is to fill in the missing sections. \n",
    "\n",
    "In \"forward\", you should compute the forward step for the operation, assuming that any parameter variables are correctly initialized. **Retain the line `self.ad.append(self)`** in each forward function at the top.\n",
    "\n",
    "In \"backward\", you should take the value of `self.output.grad` and compute `self.grad`. In terms of back propogation, `self.grad` is the error signal that will get passed backwards. To make things easier the equation/how to calculate it is given for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsmar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0.],\n",
       "       [1., 4., 5., 6.],\n",
       "       [5., 0., 0., 0.],\n",
       "       [6., 0., 0., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "test1 = np.array([1, 4, 5, 6])\n",
    "np.vstack((test1 for _ in range(10)))\n",
    "np.ones((3, 4))\n",
    "np.array((4, 4))\n",
    "p = np.zeros((4, 4))\n",
    "\n",
    "p[:, 0] = test1\n",
    "p[1, :] = test1\n",
    "\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0291b11ad012a8205eccca9e15a042c6",
     "grade": false,
     "grade_id": "cell-0c8627dd998c9016",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "'''\n",
    "AutoDiff keeps track of the operations. You do not need to edit this class.\n",
    "'''\n",
    "class AutoDiff:\n",
    "    def __init__(self):\n",
    "        self.op_stack = []\n",
    "        \n",
    "    def reset(self):\n",
    "        self.op_stack = []\n",
    "    \n",
    "    def append(self, op):\n",
    "        self.op_stack.append(op)\n",
    "        \n",
    "    def backward(self):\n",
    "        for op in reversed(self.op_stack):\n",
    "            op.backward()\n",
    "        \n",
    "class Variable:\n",
    "    '''\n",
    "    Variable represents a node in the neural network that is not a constant.\n",
    "    Weight, bias, hidden layer vectors, etc. are all Variables\n",
    "    \n",
    "    self.value - a numpy array containing the current value of the node\n",
    "    self.grad - a numpy array containing the corresponding backprop value for this node\n",
    "    '''\n",
    "    def __init__(self, value:np.array):\n",
    "        self.value = value\n",
    "        self.grad = None\n",
    "        self.requires_grad = True\n",
    "        \n",
    "    def shape(self):\n",
    "        return self.value.shape\n",
    "\n",
    "\n",
    "class MatrixMultiply:\n",
    "    '''\n",
    "    Represents a matrix multiply between variable x and variable y\n",
    "    \n",
    "    Variable x can be assumed to be a MATRIX (weights)\n",
    "    \n",
    "    Variable y can be assumed to be a VECTOR (input or hidden units of previous layer)\n",
    "    '''\n",
    "    def __init__(self, x: Variable, y: Variable, ad:AutoDiff):\n",
    "        self.param1 = x\n",
    "        self.param2 = y\n",
    "        self.output = None\n",
    "        self.ad = ad\n",
    "\n",
    "    def forward(self):\n",
    "        '''\n",
    "        calculate the matrix muliplication of self.param1 and self.param2 and store it in self.output\n",
    "            - self.output must be a Variable type\n",
    "        returns: self.output\n",
    "        '''\n",
    "        self.ad.append(self)\n",
    "        self.output = Variable(np.dot(self.param1.value, self.param2.value))\n",
    "        return self.output\n",
    "        \n",
    "\n",
    "    def backward(self):\n",
    "        '''\n",
    "        IF self.param1.requires_grad == True, \n",
    "            Calculate the derivative of self.output with respect to \n",
    "           self.param1 and store in self.param1.grad \n",
    "        \n",
    "        Likewise of self.param2\n",
    "        \n",
    "        self.param1.grad should be the following: transpose self.param2.value and stack it vertically \n",
    "                as many times as self.param1.value.shape[0]. Then pointwise multiply by self.output.grad.\n",
    "                \n",
    "        self.param2.grad should be the following: \n",
    "                perform a matrix multiplication between the transpose of param1 and self.output.grad\n",
    "                \n",
    "               \n",
    "        You don't need to return anything.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        if self.param1.requires_grad == True:\n",
    "            dy = self.output.grad\n",
    "            vec1 = self.param2.value\n",
    "            r = self.param1.shape()\n",
    "            mat1 = np.vstack((vec1.transpose() for _ in range(r[0]))).transpose()\n",
    "            #dy_mat = np.zeros(mat1.shape())\n",
    "            #for r in len(mat1.shape()[1]):\n",
    "            #    dy_mat[r, :] = dy\n",
    "            self.param1.grad = np.multiply(mat1, dy)\n",
    "            self.param1.requires_grad = False\n",
    "        if self.param2.requires_grad == True:\n",
    "            dy = self.output.grad\n",
    "            self.param2.grad = np.dot(self.param1.value.transpose(), dy)\n",
    "            self.param2.requires_grad = False\n",
    "class ReLU:\n",
    "    def __init__(self, x: Variable, ad:AutoDiff):\n",
    "        self.parameter = x\n",
    "        self.output = None\n",
    "        self.ad = ad\n",
    "\n",
    "    def forward(self):\n",
    "        '''\n",
    "        Create a new Variable self.output and store ReLU(self.parameter.value) in self.output.value\n",
    "        return self.output\n",
    "        '''\n",
    "        self.ad.append(self)\n",
    "        self.output = Variable(self.parameter.value)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self):\n",
    "        '''\n",
    "        set self.parameter.grad to be equal to self.output.grad where self.output.value > 1.\n",
    "        Otherwise, it is zero.\n",
    "        \n",
    "        You don't need to return anything.\n",
    "        '''\n",
    "        # YOUR CODE HERE\n",
    "        self.parameter.grad = np.array([0 for _ in range(self.output.shape()[0])])\n",
    "        #self.output is a variable with relus parameters value as its value. \n",
    "        for k, (i, j) in enumerate(zip(self.output.grad, self.output.value)):\n",
    "            if j > 1:\n",
    "                self.parameter.grad[k] = i\n",
    "            else:\n",
    "                self.parameter.grad[k] = 0\n",
    "            \n",
    "\n",
    "class Add:\n",
    "    '''\n",
    "    This is the only function we have that takes 2 Variables (instead of a Variable and a constant)\n",
    "    Follow the instructions for forward and backward methods.\n",
    "    '''\n",
    "    def __init__(self, x: Variable, y: Variable, ad:AutoDiff):\n",
    "        self.param1 = x\n",
    "        self.param2 = y\n",
    "        self.output = None\n",
    "        self.ad = ad\n",
    "        \n",
    "    def forward(self):\n",
    "        '''\n",
    "        Store self.param1.value + self.param2.value in self.output.value\n",
    "        return self.output\n",
    "        '''\n",
    "        self.ad.append(self)\n",
    "        self.output = Variable(self.param1.value + self.param2.value)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self):\n",
    "        '''\n",
    "        Set self.param1.grad AND self.param2.grad to the following: \n",
    "            It should be a matrix of all ones of the same size as self.parm1.value\n",
    "            multiplied pointwise by self.output.grad\n",
    "            \n",
    "        You don't need to return anything.\n",
    "        '''\n",
    "        self.param1.grad = np.ones(self.param1.shape()) * self.output.grad\n",
    "        self.param2.grad = self.param1.grad\n",
    "        \n",
    "class SoftmaxCrossEntropyLoss:\n",
    "    '''\n",
    "    Combines the softmax and cross entropy loss into one function. \n",
    "    '''\n",
    "    def __init__(self, logits: Variable, labels: np.array,  ad:AutoDiff):\n",
    "        self.logits = logits\n",
    "        self.labels = labels\n",
    "        self.output = None\n",
    "        self.ad = ad\n",
    "        \n",
    "    def forward(self):\n",
    "        '''\n",
    "        Store the loss function between the logits and labels in self.output. \n",
    "        \n",
    "        Hint: the simplest way to do this is loss = -logits[label value as index] + log(sum(exp(logits)))\n",
    "        '''\n",
    "        self.ad.append(self)\n",
    "        #self.exp_logits = np.exp(self.logits.value)\n",
    "        #self.exp_logits_sum = np.sum(self.exp_logits)\n",
    "        #losses = np.array([-1 * self.logits.value[i] + np.log(self.exp_logits_sum) for i in self.labels])\n",
    "        #self.output = Variable(losses)\n",
    "        \n",
    "        self.exp_logits = np.exp(self.logits.value)\n",
    "        self.exp_logits_sum = np.sum(self.exp_logits)\n",
    "        losses = np.array([-1.0 * self.logits.value[i] for i in self.labels]) + np.log(self.exp_logits_sum)\n",
    "        self.output = Variable(losses)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self):\n",
    "        '''\n",
    "        Calculates self.logits.grad\n",
    "            \n",
    "        This one is done for you, but fel free to verify the correctness.\n",
    "        \n",
    "        You don't need to return anything.\n",
    "        '''\n",
    "        denom = np.sum(np.exp(self.logits.value))\n",
    "        self.logits.grad = self.exp_logits/self.exp_logits_sum\n",
    "        self.logits.grad[self.labels] -= 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "01499c9cbe4a9a9c8a414166b833bf64",
     "grade": false,
     "grade_id": "cell-f94731a7119d00bf",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The below cell tests your functions. They implement the running example from the first part of the assignment, and check to make sure the values computed are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "733a7816018fbce918fa7c04d7608b37",
     "grade": true,
     "grade_id": "cell-4aa3c549fa223e80",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tsmar\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:84: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "...FFFFF\n",
      "======================================================================\n",
      "FAIL: test_multiply_op_backward (__main__.TestOperations)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-58-6e6849b0abe2>\", line 36, in test_multiply_op_backward\n",
      "    np.testing.assert_allclose(self.W0.grad, np.array([[0.05, 0.01], [-0.1, -0.02]]))\n",
      "  File \"C:\\Users\\tsmar\\Anaconda3\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\", line 1503, in assert_allclose\n",
      "    verbose=verbose, header=header, equal_nan=equal_nan)\n",
      "  File \"C:\\Users\\tsmar\\Anaconda3\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\", line 829, in assert_array_compare\n",
      "    raise AssertionError(msg)\n",
      "AssertionError: \n",
      "Not equal to tolerance rtol=1e-07, atol=0\n",
      "\n",
      "Mismatch: 50%\n",
      "Max absolute difference: 0.11\n",
      "Max relative difference: 11.\n",
      " x: array([[ 0.05, -0.1 ],\n",
      "       [ 0.01, -0.02]])\n",
      " y: array([[ 0.05,  0.01],\n",
      "       [-0.1 , -0.02]])\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_relu_backward (__main__.TestOperations)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-58-6e6849b0abe2>\", line 83, in test_relu_backward\n",
      "    np.testing.assert_allclose(op_bias.output.grad, np.array([0.1, 0.0]))\n",
      "  File \"C:\\Users\\tsmar\\Anaconda3\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\", line 1503, in assert_allclose\n",
      "    verbose=verbose, header=header, equal_nan=equal_nan)\n",
      "  File \"C:\\Users\\tsmar\\Anaconda3\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\", line 829, in assert_array_compare\n",
      "    raise AssertionError(msg)\n",
      "AssertionError: \n",
      "Not equal to tolerance rtol=1e-07, atol=0\n",
      "\n",
      "Mismatch: 50%\n",
      "Max absolute difference: 0.1\n",
      "Max relative difference: nan\n",
      " x: array([0, 0])\n",
      " y: array([0.1, 0. ])\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_relu_op (__main__.TestOperations)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-58-6e6849b0abe2>\", line 71, in test_relu_op\n",
      "    np.testing.assert_allclose(result.value, np.array([0.649, 0.0]))\n",
      "  File \"C:\\Users\\tsmar\\Anaconda3\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\", line 1503, in assert_allclose\n",
      "    verbose=verbose, header=header, equal_nan=equal_nan)\n",
      "  File \"C:\\Users\\tsmar\\Anaconda3\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\", line 829, in assert_array_compare\n",
      "    raise AssertionError(msg)\n",
      "AssertionError: \n",
      "Not equal to tolerance rtol=1e-07, atol=0\n",
      "\n",
      "Mismatch: 50%\n",
      "Max absolute difference: 0.272\n",
      "Max relative difference: inf\n",
      " x: array([ 0.649, -0.272])\n",
      " y: array([0.649, 0.   ])\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_softmax_backward (__main__.TestOperations)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-58-6e6849b0abe2>\", line 105, in test_softmax_backward\n",
      "    np.testing.assert_allclose(op.output.grad, np.array([0.37072916, 0.43707367, -0.807803]),rtol=1e-3)\n",
      "  File \"C:\\Users\\tsmar\\Anaconda3\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\", line 1503, in assert_allclose\n",
      "    verbose=verbose, header=header, equal_nan=equal_nan)\n",
      "  File \"C:\\Users\\tsmar\\Anaconda3\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\", line 829, in assert_array_compare\n",
      "    raise AssertionError(msg)\n",
      "AssertionError: \n",
      "Not equal to tolerance rtol=0.001, atol=0\n",
      "\n",
      "Mismatch: 100%\n",
      "Max absolute difference: 0.1118168\n",
      "Max relative difference: 0.30161317\n",
      " x: array([ 0.482546,  0.374213, -0.856759])\n",
      " y: array([ 0.370729,  0.437074, -0.807803])\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_softmax_op (__main__.TestOperations)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"<ipython-input-58-6e6849b0abe2>\", line 93, in test_softmax_op\n",
      "    np.testing.assert_allclose(result.value, 1.6492335123)\n",
      "  File \"C:\\Users\\tsmar\\Anaconda3\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\", line 1503, in assert_allclose\n",
      "    verbose=verbose, header=header, equal_nan=equal_nan)\n",
      "  File \"C:\\Users\\tsmar\\Anaconda3\\lib\\site-packages\\numpy\\testing\\_private\\utils.py\", line 829, in assert_array_compare\n",
      "    raise AssertionError(msg)\n",
      "AssertionError: \n",
      "Not equal to tolerance rtol=1e-07, atol=0\n",
      "\n",
      "Mismatch: 100%\n",
      "Max absolute difference: 0.2939956\n",
      "Max relative difference: 0.17826196\n",
      " x: array([1.943229])\n",
      " y: array(1.649234)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 8 tests in 0.022s\n",
      "\n",
      "FAILED (failures=5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1cce0814550>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add Op Test\n",
    "import unittest\n",
    "import numpy as np\n",
    "\n",
    "class TestOperations(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        np.random.seed(1)\n",
    "        a = np.random.randn(2,2).round(2)\n",
    "        b = np.random.randn(3,2).round(2)\n",
    "        self.ad = AutoDiff()\n",
    "        self.xnp = np.array([0.5, 0.1])\n",
    "        self.w0np = np.array(a)\n",
    "        self.w1np = np.array(b)\n",
    "        self.b0np = np.array([-0.1, 0.1])\n",
    "        self.b1np = np.array([0.5, 0.1, 0.2])\n",
    "        self.x_i = Variable(self.xnp)\n",
    "        self.x_i.requires_grad = False\n",
    "        self.W0 = Variable(self.w0np)\n",
    "        self.W1 = Variable(self.w1np)\n",
    "        self.b0 = Variable(self.b0np)\n",
    "        self.b1 = Variable(self.b1np)\n",
    "\n",
    "    def test_multiply_op(self):\n",
    "        op = MatrixMultiply(self.W0, self.x_i, self.ad)\n",
    "        result = op.forward()\n",
    "        np.testing.assert_allclose(result.value, np.array([0.749, -0.372]))\n",
    "        # Tests you did not delete the ad line\n",
    "        self.assertEqual(len(self.ad.op_stack), 1)\n",
    "        \n",
    "    def test_multiply_op_backward(self):\n",
    "        op = MatrixMultiply(self.W0, self.x_i, self.ad)\n",
    "        result = op.forward()\n",
    "        result.grad = np.array([0.1, -0.2])\n",
    "        self.ad.backward()\n",
    "        assert(self.x_i.grad is None)\n",
    "        np.testing.assert_allclose(self.W0.grad, np.array([[0.05, 0.01], [-0.1, -0.02]]))\n",
    "        \n",
    "        # Check grad w.r.t. vector\n",
    "        self.x_i.requires_grad = True\n",
    "        op = MatrixMultiply(self.W0, self.x_i, self.ad)\n",
    "        result = op.forward()\n",
    "        result.grad = np.ones_like(result.value)\n",
    "        self.ad.backward()\n",
    "        assert(self.x_i.grad is not None)\n",
    "        np.testing.assert_allclose(self.x_i.grad, np.array([0.268, 0.153]))\n",
    "\n",
    "    def test_add_op(self):\n",
    "        op = MatrixMultiply(self.W0, self.x_i, self.ad)\n",
    "        result = op.forward()\n",
    "        op_bias = Add(result, self.b0, self.ad)\n",
    "        result = op_bias.forward()\n",
    "        np.testing.assert_allclose(result.value, np.array([0.649, -0.272]))\n",
    "        # Tests you did not delete the ad line\n",
    "        self.assertEqual(len(self.ad.op_stack), 2)\n",
    "        \n",
    "    def test_add_backward(self):\n",
    "        op = MatrixMultiply(self.W0, self.x_i, self.ad)\n",
    "        op_bias = Add(op.forward(), self.b0, self.ad)\n",
    "        result = op_bias.forward()\n",
    "        result.grad = np.array([0.1, -0.2])\n",
    "        self.ad.backward()\n",
    "        np.testing.assert_allclose(self.b0.grad, np.array([0.1, -0.2]))\n",
    "        \n",
    "    def test_relu_op(self):\n",
    "        op = MatrixMultiply(self.W0, self.x_i, self.ad)\n",
    "        result = op.forward()\n",
    "        op_bias = Add(result, self.b0, self.ad)\n",
    "        result = op_bias.forward()        \n",
    "        op_relu = ReLU(result, self.ad)\n",
    "        result = op_relu.forward()\n",
    "        np.testing.assert_allclose(result.value, np.array([0.649, 0.0]))\n",
    "\n",
    "        # Tests you did not delete the ad line\n",
    "        self.assertEqual(len(self.ad.op_stack), 3)\n",
    "        \n",
    "    def test_relu_backward(self):\n",
    "        op = MatrixMultiply(self.W0, self.x_i, self.ad)\n",
    "        op_bias = Add(op.forward(), self.b0, self.ad)\n",
    "        op_relu = ReLU(op_bias.forward(), self.ad)\n",
    "        result = op_relu.forward()\n",
    "        result.grad = np.array([0.1, -0.2])\n",
    "        self.ad.backward()\n",
    "        np.testing.assert_allclose(op_bias.output.grad, np.array([0.1, 0.0]))\n",
    "        \n",
    "    def test_softmax_op(self):\n",
    "        op = MatrixMultiply(self.W0, self.x_i, self.ad)\n",
    "        op = Add(op.forward(), self.b0, self.ad)\n",
    "        op = ReLU(op.forward(), self.ad)\n",
    "        op = MatrixMultiply(self.W1, op.forward(), self.ad)\n",
    "        op = Add(op.forward(), self.b1, self.ad)\n",
    "        op_sm = SoftmaxCrossEntropyLoss(op.forward(), np.array([2,], dtype=np.long), self.ad)\n",
    "        result = op_sm.forward()\n",
    "        np.testing.assert_allclose(result.value, 1.6492335123)\n",
    "        self.assertEqual(len(self.ad.op_stack), 6)\n",
    "        \n",
    "    def test_softmax_backward(self):\n",
    "        op = MatrixMultiply(self.W0, self.x_i, self.ad)\n",
    "        op = Add(op.forward(), self.b0, self.ad)\n",
    "        op = ReLU(op.forward(), self.ad)\n",
    "        op = MatrixMultiply(self.W1, op.forward(), self.ad)\n",
    "        op = Add(op.forward(), self.b1, self.ad)\n",
    "        op_sm = SoftmaxCrossEntropyLoss(op.forward(), np.array([2,], dtype=np.long), self.ad)\n",
    "        result = op_sm.forward()\n",
    "        self.ad.backward()\n",
    "        np.testing.assert_allclose(op.output.grad, np.array([0.37072916, 0.43707367, -0.807803]),rtol=1e-3)\n",
    "        np.testing.assert_allclose(self.W0.grad, np.array([[0.41227283, 0.08245457], [0.0, 0.0]]), rtol=1e-3)\n",
    "        np.testing.assert_allclose(self.b0.grad, np.array([0.824546, 0.0]), rtol=1e-3)\n",
    "        \n",
    "unittest.main(argv=[''], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c28b3841c2f2a69f83a87ebd9fd3024b",
     "grade": false,
     "grade_id": "cell-6216b195753aeeae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part 3: Creating a Network (5 points)\n",
    "\n",
    "You will now build the class Neural Network, which uses the builting blocks above to create a MLP with configurable options. Our `NeuralNetwork` class takes the following parameters:\n",
    "\n",
    "1. `input_size` - the dimensionality of the input. In this notebook, we will always use 2.\n",
    "2. `layer_widths` - a tuple or list of widths. If the list is (3,4,3), then the network will consist of 3 layers. The sizes of the outputs of the layers are given by the list. The final number is the logit output layer and should match the number of classes.\n",
    "\n",
    "Fill in the network below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae646177d2d92bcf2dd8e3ac07b1f301",
     "grade": false,
     "grade_id": "cell-b36b171b8eb9a540",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_size=2, layer_widths = [2,3]):\n",
    "        '''\n",
    "        layer_widths : the size of the hidden layers. The last number should be equal to the number of classes.\n",
    "        '''\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.ad = AutoDiff()\n",
    "        '''\n",
    "        Here we build the network by adding a series of weights an biases to the list.\n",
    "        '''\n",
    "        last_width = input_size\n",
    "        for h_width in layer_widths:\n",
    "            weight = Variable(np.zeros((h_width, last_width)))\n",
    "            self.weights.append(weight)\n",
    "            last_width = h_width\n",
    "            bias = Variable(np.zeros(h_width))\n",
    "            self.biases.append(bias)\n",
    "            \n",
    "        '''\n",
    "        Initializes the weights\n",
    "        '''\n",
    "        self.init_weights_default(self.weights, self.biases)\n",
    "    \n",
    "        \n",
    "    def init_weights_default(self, weights:list, biases: list, variance=0.01):\n",
    "        '''\n",
    "        returns a set of weights used to initialize a weights matrix\n",
    "        weights should be drawn from a gaussian distribution with variance `variance`.\n",
    "        variance: float\n",
    "        shape: tuple or list that describes the shape of the required tensor\n",
    "        '''\n",
    "        for w in weights:\n",
    "            w.value = np.random.randn(*w.value.shape)*variance\n",
    "\n",
    "        for b in biases:\n",
    "            b.value = np.zeros_like(b.value)\n",
    "        \n",
    "    def forward(self, x : np.array, y :np.array = None):\n",
    "        '''\n",
    "        Accepts input data and performs forward pass\n",
    "        \n",
    "        If y is not None, then it will calculate the loss, otherwise it runs None for the loss.\n",
    "        \n",
    "        Returns:\n",
    "            loss: the Variable containing the loss value returned from SoftmaxCrossEntropyLoss\n",
    "            logits: the Variable containing the hidden units prior to the loss\n",
    "            activations: a list containing the Variable for the hidden units post-relu at each layer\n",
    "        '''\n",
    "        # Now you will do the forward step. Part of it is started for you.\n",
    "        # Remember the last layer should not get a ReLU activation\n",
    "        self.ad.reset()\n",
    "        last = Variable(x)\n",
    "        activations = []\n",
    "        for idx, (weight, bias) in enumerate(zip(self.weights, self.biases)):\n",
    "            op = MatrixMultiply(weight, last, self.ad)\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "        logits = last\n",
    "        \n",
    "        loss = None\n",
    "        if y is not None:\n",
    "            # Calculate the loss using the SoftmaxCrossEntropyLoss\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "        return loss, logits, activations\n",
    "    \n",
    "    \n",
    "    def apply_gradients(self, learning_rate):\n",
    "        '''\n",
    "        Fill in the code below to apply the gradients using the learning_rate\n",
    "        '''\n",
    "        for w in self.weights:\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "        for b in self.biases:\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "    \n",
    "    \n",
    "    def backward(self):\n",
    "        '''\n",
    "        Performs back propogation.\n",
    "        You do not have to edit this.\n",
    "        '''\n",
    "        self.ad.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7558f23396a238616d07f4c2b8d12598",
     "grade": true,
     "grade_id": "cell-f7e47030bff9d85f",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This tests your implementation.\n",
    "import time\n",
    "import unittest\n",
    "\n",
    "class TestNetwork:\n",
    "    def test_network(self):\n",
    "        # Conducts our usual test so far by doing the forward then the reverse propogation\n",
    "        np.random.seed(0)\n",
    "        sample = np.array([0.5, 0.1])\n",
    "        label = np.array(2, dtype=np.long)\n",
    "        bias1np = np.array([-0.1, 0.1])\n",
    "        bias2np = np.array([0.5, 0.1, 0.2])\n",
    "        model = NeuralNetwork(input_size = 2, layer_widths =[2,3])\n",
    "        model.biases[0].value = bias1np\n",
    "        model.biases[1].value = bias2np\n",
    "        loss, logits = model.forward(sample, label)\n",
    "        model.backward()\n",
    "        model.apply_gradients(0.01)\n",
    "        np.testing.assert_allclose(model.weights[0].grad, np.array([[0.41227283, 0.08245457], [0.0, 0.0]]), rtol=1e-3)\n",
    "        np.testing.assert_allclose(model.biases[0].grad, np.array([0.824546, 0.0]), rtol=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5932307f7a5ba29d34644b42a96800e7",
     "grade": false,
     "grade_id": "cell-ae307331107df4c7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Part Three: Training the Model \n",
    "\n",
    "You will now implement the training loop. First, we need some data. We will generate some non-linearly seperable data which are colloquially known as \"swiss rolls\". We'll use an increasing number of classes to make the task harder. (To test the network, first train it on `dataset1`, then `dataset2`, and so on. We will use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "698b0a2fe578ae63db1476bbf95fe405",
     "grade": false,
     "grade_id": "cell-8ca36f43683a3a90",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def generate_sample_data(num_points = 100, num_class=3):\n",
    "    '''\n",
    "    Generate some cool data. Reproduced from [reference witheld]\n",
    "    \n",
    "    num_points is num points per class\n",
    "    '''\n",
    "    D = 2 \n",
    "    K = num_class\n",
    "    X = np.zeros((num_points*K,D)) # data matrix (each row = single example)\n",
    "    y = np.zeros(num_points*K, dtype=np.long) # class labels\n",
    "    for j in range(K):\n",
    "      ix = range(num_points*j,num_points*(j+1))\n",
    "      r = np.linspace(0.0,1,num_points) # radius\n",
    "      t = np.linspace(j*4,(j+1)*4,num_points) + np.random.randn(num_points)*0.2 # theta\n",
    "      X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "      y[ix] = j\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.75)\n",
    "    return {\n",
    "        'train': {\n",
    "            'data': X_train,\n",
    "            'labels': y_train \n",
    "        },\n",
    "        'valid': {\n",
    "            'data': X_valid,\n",
    "            'labels': y_valid\n",
    "        }\n",
    "    }\n",
    "\n",
    "def visualize(data :dict, model=None, title:str=\"\"):\n",
    "    if model is not None:\n",
    "        plot_step=0.25\n",
    "        x_min, x_max = data[\"data\"][:, 0].min() - 1, data[\"data\"][:, 0].max() + 1\n",
    "        y_min, y_max = data[\"data\"][:, 1].min() - 1, data[\"data\"][:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                             np.arange(y_min, y_max, plot_step))\n",
    "        predictions = []\n",
    "        for x in np.c_[xx.ravel(), yy.ravel()]:\n",
    "            _, p, _ = model.forward(x)\n",
    "            predictions.append(np.argmax(p.value))\n",
    "        prediction_color = np.array(predictions).reshape(xx.shape)\n",
    "        cs = plt.contourf(xx, yy, prediction_color, cmap=plt.cm.Paired)\n",
    "        \n",
    "    plt.scatter(data[\"data\"][:, 0], data[\"data\"][:, 1], c=data[\"labels\"],edgecolors='black',cmap=plt.cm.Paired)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "dataset1 = generate_sample_data(100, 1)\n",
    "dataset2 = generate_sample_data(100, 2)\n",
    "dataset3 = generate_sample_data(100, 3)\n",
    "\n",
    "visualize(dataset1[\"train\"], title=\"dataset1\")\n",
    "visualize(dataset2[\"train\"], title=\"dataset2\")\n",
    "visualize(dataset3[\"train\"], title=\"dataset3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "270ad5bbc1f3b475f0477776c8226473",
     "grade": false,
     "grade_id": "cell-9a37540f42f9195f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Write the `train` function (5 points)\n",
    "\n",
    "Follow the instructions below to fill in `train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3053cff032f01ef243448a4ef3a61a6",
     "grade": true,
     "grade_id": "cell-d94d14655128c2ac",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def train(dataset, model, epochs=1, learning_rate=0.001):\n",
    "    '''\n",
    "    train function acccepts a dataset, model, and some hyperparameters. \n",
    "    \n",
    "    Conducts training for the specified number of epochs.\n",
    "    \n",
    "    Each epoch is a loop over the training data. \n",
    "    \n",
    "    Call model.forward, model.backward, and model.apply_gradients once for each data point in the dataset\n",
    "    \n",
    "    returns:\n",
    "        a dictionary of\n",
    "            {\n",
    "                \"loss\": the loss reported by the model each time model.forward is called.\n",
    "                \"validation_accyracy\": the validation accuracy at the end of each epoch,\n",
    "                \"activations\": A list containing:\n",
    "                  the value of \"activations\" returned by model.forward. It should only be stored in this list\n",
    "                  once per epoch (after completing the training loop)\n",
    "            }\n",
    "    '''\n",
    "    history = {\n",
    "        \"loss\": [],\n",
    "        \"validation_accuracy\": [],\n",
    "        \"activations\": []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(1, epochs+1):\n",
    "        for idx, (sample_x, sample_y) in enumerate(zip(dataset[\"train\"][\"data\"], dataset[\"train\"][\"labels\"])):\n",
    "            '''\n",
    "            Provide the implementation for the forward, backward, and gradient application steps here.\n",
    "            Be sure to update the history dictionary.\n",
    "            '''\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "        history['activations'].append(activations)\n",
    "        \n",
    "        '''\n",
    "        Provide the implementation for the validation loop. Do not do training (do not call model.backward)\n",
    "        \n",
    "        After your loop, we calculate the accuracy and store it in the history dictionary.\n",
    "        '''\n",
    "        predictions = []\n",
    "        for idx, (sample_x, sample_y) in enumerate(zip(dataset[\"valid\"][\"data\"], dataset[\"valid\"][\"labels\"])):\n",
    "            # YOUR CODE HERE\n",
    "            raise NotImplementedError()\n",
    "        history[\"validation_accuracy\"].append(accuracy_score(predictions, dataset[\"valid\"][\"labels\"]))\n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "daa5c15d0cf1f5c9264755ef44265d6a",
     "grade": false,
     "grade_id": "cell-7dda8327acb5ad96",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Simple Convergence Test with `dataset1` (2 pt)\n",
    "\n",
    "To test if things converge, we will use `dataset1`, which only has 1 class, to ensure our `train` function is working. We expect the neural network should converge rapidly to 0-loss using a reasonable learning rate.\n",
    "\n",
    "In the following cell, create a `Neural Network` model with a modest `layer_widths` parameter, `[5,3]`. \n",
    "\n",
    "- Call the `train` function of the model on `dataset1` in a loop, using a different learning rate for each iteration. The values you should use are `1.0, 0.1, 0.001`. Use only 1 epoch.\n",
    "- Plot the history of the loss for each learning rate (plot all three lines on the same plot).\n",
    "- Use the `visualize` given for you above to plot the visualization of the trained model against the training data. You should see the entire plan is classified as 1 class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "76be27bac8964b7c56875c4bcb21d462",
     "grade": true,
     "grade_id": "cell-fa2b4004154b32bd",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "learning_rates = [1.0, 0.1, 0.01]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6bae98a717eb229cb0b8f64638348d6f",
     "grade": false,
     "grade_id": "cell-837a42103fdafd53",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Train the Models on the More Difficult dataset2 and dataset3 (5 pts)\n",
    "\n",
    "Write code below to train a NeuralNetwork for each dataset2 and dataset3 \n",
    "\n",
    "    - Train a maximum of 5 different models, for a maximum of 10 epochs each.\n",
    "    \n",
    "    - Your models should vary in the layer_widths parameter using a different combination of total number of layers and width of the largest layer. Good values to try might keep the total width constant, but increase the number of layers, such as [10]*10+[3] or very wide, shallow networks, like [100,100,3].\n",
    "    \n",
    "    - Graph 1: Plot the loss history returned by the train function for each model\n",
    "    \n",
    "    - Graph 2: Plot the validation accuracy history returned by the `train` function for each model.\n",
    "    \n",
    "    - Graph 3: Plot the mean and standard deviation of the activations. The x axis will be layer number, and the y axis will be mean or standard deviation. You should have 1 line per epoch. \n",
    "    \n",
    "    - Graph 4: Use the visualize function provided to visualize the model against the training data\n",
    "    \n",
    "Obviously, this is going to produce many plots for 5 possible models. Try to keep things compact by using subplots as much as possible and comining figures where appropriate.\n",
    "\n",
    "For deeper networks, you will need to change the default initialization. One method is provided for you.\n",
    "\n",
    "In the Markdown cell below, comment on your findings regarding the following:\n",
    "1. How does depth or width of the model affect trainability (the ability of the model to converge in a reasonable number of epochs). Does using the provided initialization help train deeper models? Why or why not?\n",
    "2. How does depth or width affect performance (by looking at validation accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34c3f1bbe8e739a80d223921c204d214",
     "grade": true,
     "grade_id": "cell-0a4cb49e1c1139ce",
     "locked": false,
     "points": 5,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def normal_init(model):\n",
    "    for w in model.weights:\n",
    "        normval = np.sqrt(2/(w.value.shape[0]+w.value.shape[1]))\n",
    "        w.value = np.random.randn(*w.value.shape)*normval\n",
    "    for b in model.biases:\n",
    "        b.value = np.zeros_like(b.value)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91042046f44027dd6370389b873ec4eb",
     "grade": true,
     "grade_id": "cell-ffe4c7b741ce9e08",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
